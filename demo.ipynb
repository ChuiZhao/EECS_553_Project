{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fab6b7d5",
   "metadata": {},
   "source": [
    "# Demo\n",
    "Examples of how to use this repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea74aa3",
   "metadata": {},
   "source": [
    "These are the commands for VPT with `p=50`:\n",
    "\n",
    "VPT-deep\n",
    "```bash\n",
    "    --config-file configs/prompt/*.yaml\n",
    "    MODEL.TRANSFER_TYPE \"prompt\" \\\n",
    "    MODEL.PROMPT.DEEP \"True\" \\\n",
    "    MODEL.PROMPT.NUM_TOKENS \"50\" \\\n",
    "    MODEL.PROMPT.DROPOUT \"0.0\" \n",
    "```\n",
    "   \n",
    "VPT-shallow (we don't use dropout for VPT-shallow)\n",
    "```bash\n",
    "    --config-file configs/prompt/*.yaml\n",
    "    MODEL.TRANSFER_TYPE \"prompt\" \\\n",
    "    MODEL.PROMPT.DEEP \"False\" \\\n",
    "    MODEL.PROMPT.NUM_TOKENS \"50\" \\\n",
    "    MODEL.PROMPT.DROPOUT \"0.0\" \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0825e402",
   "metadata": {},
   "source": [
    "Other transfer protocols presented in the paper:\n",
    "\n",
    "Full\n",
    "```bash\n",
    "    --config-file configs/finetune/*.yaml\n",
    "```\n",
    "\n",
    "Head-oriented methods:\n",
    "\n",
    "- Linear:\n",
    "```bash\n",
    "    --config-file configs/linear/*.yaml\n",
    "```\n",
    "\n",
    "- MLP-3 (3 layer MLP):\n",
    "```bash\n",
    "    --config-file configs/linear/*.yaml \\\n",
    "    MODEL.MLP_NUM \"2\"\n",
    "```\n",
    "\n",
    "- Partial-1:\n",
    "```bash\n",
    "    --config-file configs/finetune/*.yaml \\\n",
    "    MODEL.TRANSFER_TYPE \"partial-1\"\n",
    "```\n",
    "\n",
    "\n",
    "Backbone-oriented methods:\n",
    "\n",
    "- Sidetune:\n",
    "```bash\n",
    "    --config-file configs/linear/*.yaml\n",
    "    MODEL.TRANSFER_TYPE  \"side\" \n",
    "```\n",
    "\n",
    "- Bias: \n",
    "```bash\n",
    "    --config-file configs/finetune/*.yaml \\\n",
    "    MODEL.TRANSFER_TYPE \"tinytl-bias\"\n",
    "```\n",
    "\n",
    "- Adapters with `r=128`:\n",
    "```bash\n",
    "    --config-file configs/finetune/*.yaml\n",
    "    MODEL.ADAPTER.REDUCATION_FACTOR \"128\"\n",
    "    MODEL.TRANSFER_TYPE \"adapter\" \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a755f5a",
   "metadata": {},
   "source": [
    "##  train.py\n",
    "The main script is `train.py`. Note for VTAB data, this script handles the final runs with 1000 training data. See `tune_vtab.py` for the full tune + final runs settings. Here are some examples.\n",
    "\n",
    "Note: it's recommended to directly use terminal for these command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45a55605-5ada-4f04-9e7f-662529e77b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yourenz/.conda/envs/prompt/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d77e0e-24b7-4e03-a70c-d8ed251cd965",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "source activate prompt\n",
    "\n",
    "model_root=/gpfs/accounts/eecs598s010w23_class_root/eecs598s010w23_class/shared_data/yourenz_group/pretrained/\n",
    "data_path=/gpfs/accounts/eecs598s010w23_class_root/eecs598s010w23_class/shared_data/yourenz_group/datasets/cub\n",
    "output_dir=/home/yourenz/SynergisticEfficiency/exps\n",
    "\n",
    "python train.py \\\n",
    "        --config-file configs/prompt/cub.yaml \\\n",
    "        MODEL.TYPE \"vit\" \\\n",
    "        DATA.BATCH_SIZE \"64\" \\\n",
    "        MODEL.PROMPT.NUM_TOKENS \"100\" \\\n",
    "        MODEL.PROMPT.DEEP \"True\" \\\n",
    "        MODEL.PROMPT.DROPOUT \"0.1\" \\\n",
    "        DATA.FEATURE \"sup_vitti16_imagenet21k\" \\\n",
    "        SEED 42 \\\n",
    "        MODEL.MODEL_ROOT \"${model_root}\" \\\n",
    "        DATA.DATAPATH \"${data_path}\" \\\n",
    "        OUTPUT_DIR \"${output_dir}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c5be87",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# launch final training with five random seeds for VTAB-dmlab, sun397 and eurosat. The hyperparameters are the same from our paper.\n",
    "model_root=<MODEL_ROOT>\n",
    "data_path=<DATA_PATH>\n",
    "output_dir=<OUTPUT_DIR>\n",
    "        \n",
    "# vtab-structured: dmlab\n",
    "# base_lr = 1.0\n",
    "# lr = base_lr / 256 * cfg.DATA.BATCH_SIZE\n",
    "for seed in \"42\" \"44\" \"82\" \"100\" \"800\"; do\n",
    "    python train.py \\\n",
    "        --config-file configs/prompt/cub.yaml \\\n",
    "        MODEL.TYPE \"vit\" \\\n",
    "        DATA.BATCH_SIZE \"64\" \\\n",
    "        MODEL.PROMPT.NUM_TOKENS \"100\" \\\n",
    "        MODEL.PROMPT.DEEP \"True\" \\\n",
    "        MODEL.PROMPT.DROPOUT \"0.1\" \\\n",
    "        DATA.FEATURE \"sup_vitb16_imagenet21k\" \\\n",
    "        DATA.NAME \"vtab-dmlab\" \\\n",
    "        DATA.NUMBER_CLASSES \"6\" \\\n",
    "        SOLVER.BASE_LR \"0.25\" \\\n",
    "        SOLVER.WEIGHT_DECAY \"0.001\" \\\n",
    "        SEED ${seed} \\\n",
    "        MODEL.MODEL_ROOT \"${model_root}\" \\\n",
    "        DATA.DATAPATH \"${data_path}\" \\\n",
    "        OUTPUT_DIR \"${output_dir}/seed${seed}\" \n",
    "done\n",
    "\n",
    "# vtab-natural: sun397\n",
    "# base_lr = 25\n",
    "# lr = base_lr / 256 * cfg.DATA.BATCH_SIZE\n",
    "for seed in \"42\" \"44\" \"82\" \"100\" \"800\"; do\n",
    "    python train.py \\\n",
    "        --config-file configs/prompt/cub.yaml \\\n",
    "        MODEL.TYPE \"vit\" \\\n",
    "        DATA.BATCH_SIZE \"128\" \\\n",
    "        MODEL.PROMPT.NUM_TOKENS \"5\" \\\n",
    "        MODEL.PROMPT.DEEP \"True\" \\\n",
    "        MODEL.PROMPT.DROPOUT \"0.1\" \\\n",
    "        DATA.FEATURE \"sup_vitb16_imagenet21k\" \\\n",
    "        DATA.NAME \"vtab-sun397\" \\\n",
    "        DATA.NUMBER_CLASSES \"397\" \\\n",
    "        SOLVER.BASE_LR \"12.5\" \\\n",
    "        SOLVER.WEIGHT_DECAY \"0.0001\" \\\n",
    "        SOLVER.TOTAL_EPOCH \"100\" \\\n",
    "        SEED ${seed} \\\n",
    "        MODEL.MODEL_ROOT \"${model_root}\" \\\n",
    "        DATA.DATAPATH \"${data_path}\" \\\n",
    "        OUTPUT_DIR \"${output_dir}/seed${seed}\" \n",
    "done\n",
    "\n",
    "# vtab-specialized: vtab-eurosat\n",
    "# base_lr = 1\n",
    "# lr = base_lr / 256 * cfg.DATA.BATCH_SIZE\n",
    "for seed in \"42\" \"44\" \"82\" \"100\" \"800\"; do\n",
    "    python train.py \\\n",
    "        --config-file configs/prompt/cub.yaml \\\n",
    "        MODEL.TYPE \"vit\" \\\n",
    "        DATA.BATCH_SIZE \"64\" \\\n",
    "        MODEL.PROMPT.NUM_TOKENS \"100\" \\\n",
    "        MODEL.PROMPT.DEEP \"True\" \\\n",
    "        MODEL.PROMPT.DROPOUT \"0.1\" \\\n",
    "        DATA.FEATURE \"sup_vitb16_imagenet21k\" \\\n",
    "        DATA.NAME \"vtab-eurosat\" \\\n",
    "        DATA.NUMBER_CLASSES \"10\" \\\n",
    "        SOLVER.BASE_LR \"0.25\" \\\n",
    "        SOLVER.WEIGHT_DECAY \"0.001\" \\\n",
    "        SOLVER.TOTAL_EPOCH \"100\" \\\n",
    "        SEED ${seed} \\\n",
    "        MODEL.MODEL_ROOT \"${model_root}\" \\\n",
    "        DATA.DATAPATH \"${data_path}\" \\\n",
    "        OUTPUT_DIR \"${output_dir}/seed${seed}\" \n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63977e8e",
   "metadata": {},
   "source": [
    "## Get results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14a527c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "from src.utils.vis_utils import get_df, average_df\n",
    "\n",
    "LOG_NAME = \"logs.txt\"\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8a81bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seed42: 100%|██████████| 3/3 [00:00<00:00, 241.66it/s]\n",
      "seed42: 100%|██████████| 3/3 [00:00<00:00, 240.13it/s]\n",
      "seed42: 100%|██████████| 3/3 [00:00<00:00, 229.10it/s]\n",
      "seed44: 100%|██████████| 3/3 [00:00<00:00, 223.63it/s]\n",
      "seed44: 100%|██████████| 3/3 [00:00<00:00, 265.73it/s]\n",
      "seed44: 100%|██████████| 3/3 [00:00<00:00, 232.97it/s]\n",
      "seed82: 100%|██████████| 3/3 [00:00<00:00, 221.82it/s]\n",
      "seed82: 100%|██████████| 3/3 [00:00<00:00, 224.52it/s]\n",
      "seed82: 100%|██████████| 3/3 [00:00<00:00, 258.04it/s]\n",
      "seed100: 100%|██████████| 3/3 [00:00<00:00, 222.09it/s]\n",
      "seed100: 100%|██████████| 3/3 [00:00<00:00, 215.91it/s]\n",
      "seed100: 100%|██████████| 3/3 [00:00<00:00, 236.46it/s]\n",
      "seed800: 100%|██████████| 3/3 [00:00<00:00, 236.60it/s]\n",
      "seed800: 100%|██████████| 3/3 [00:00<00:00, 229.92it/s]\n",
      "seed800: 100%|██████████| 3/3 [00:00<00:00, 252.79it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>feature</th>\n",
       "      <th>lr</th>\n",
       "      <th>wd</th>\n",
       "      <th>total_params</th>\n",
       "      <th>tuned_params</th>\n",
       "      <th>tuned / total (%)</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>l-val_top1</th>\n",
       "      <th>l-test_top1</th>\n",
       "      <th>best_epoch</th>\n",
       "      <th>file</th>\n",
       "      <th>total_time</th>\n",
       "      <th>seed</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vtab-dmlab</td>\n",
       "      <td>sup_vitb16_imagenet21k</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>86724870</td>\n",
       "      <td>926214</td>\n",
       "      <td>1.0680</td>\n",
       "      <td>64</td>\n",
       "      <td>100.0</td>\n",
       "      <td>46.88</td>\n",
       "      <td>76 | 100</td>\n",
       "      <td>/fsx/menglin/experiments/2022prompt/output/rel...</td>\n",
       "      <td>0 days 01:06:03</td>\n",
       "      <td>42</td>\n",
       "      <td>VPT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vtab-eurosat</td>\n",
       "      <td>sup_vitb16_imagenet21k</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>86727946</td>\n",
       "      <td>929290</td>\n",
       "      <td>1.0715</td>\n",
       "      <td>64</td>\n",
       "      <td>100.0</td>\n",
       "      <td>96.00</td>\n",
       "      <td>38 | 100</td>\n",
       "      <td>/fsx/menglin/experiments/2022prompt/output/rel...</td>\n",
       "      <td>0 days 00:44:57</td>\n",
       "      <td>42</td>\n",
       "      <td>VPT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vtab-sun397</td>\n",
       "      <td>sup_vitb16_imagenet21k</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>86150029</td>\n",
       "      <td>351373</td>\n",
       "      <td>0.4079</td>\n",
       "      <td>128</td>\n",
       "      <td>100.0</td>\n",
       "      <td>52.57</td>\n",
       "      <td>14 | 100</td>\n",
       "      <td>/fsx/menglin/experiments/2022prompt/output/rel...</td>\n",
       "      <td>0 days 00:44:57</td>\n",
       "      <td>42</td>\n",
       "      <td>VPT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vtab-dmlab</td>\n",
       "      <td>sup_vitb16_imagenet21k</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>86724870</td>\n",
       "      <td>926214</td>\n",
       "      <td>1.0680</td>\n",
       "      <td>64</td>\n",
       "      <td>99.5</td>\n",
       "      <td>46.25</td>\n",
       "      <td>85 | 100</td>\n",
       "      <td>/fsx/menglin/experiments/2022prompt/output/rel...</td>\n",
       "      <td>0 days 01:06:09</td>\n",
       "      <td>44</td>\n",
       "      <td>VPT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vtab-eurosat</td>\n",
       "      <td>sup_vitb16_imagenet21k</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>86727946</td>\n",
       "      <td>929290</td>\n",
       "      <td>1.0715</td>\n",
       "      <td>64</td>\n",
       "      <td>100.0</td>\n",
       "      <td>96.54</td>\n",
       "      <td>41 | 100</td>\n",
       "      <td>/fsx/menglin/experiments/2022prompt/output/rel...</td>\n",
       "      <td>0 days 00:44:38</td>\n",
       "      <td>44</td>\n",
       "      <td>VPT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vtab-sun397</td>\n",
       "      <td>sup_vitb16_imagenet21k</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>86150029</td>\n",
       "      <td>351373</td>\n",
       "      <td>0.4079</td>\n",
       "      <td>128</td>\n",
       "      <td>100.0</td>\n",
       "      <td>49.03</td>\n",
       "      <td>32 | 100</td>\n",
       "      <td>/fsx/menglin/experiments/2022prompt/output/rel...</td>\n",
       "      <td>0 days 00:44:53</td>\n",
       "      <td>44</td>\n",
       "      <td>VPT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vtab-dmlab</td>\n",
       "      <td>sup_vitb16_imagenet21k</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>86724870</td>\n",
       "      <td>926214</td>\n",
       "      <td>1.0680</td>\n",
       "      <td>64</td>\n",
       "      <td>100.0</td>\n",
       "      <td>46.14</td>\n",
       "      <td>65 | 100</td>\n",
       "      <td>/fsx/menglin/experiments/2022prompt/output/rel...</td>\n",
       "      <td>0 days 01:04:18</td>\n",
       "      <td>82</td>\n",
       "      <td>VPT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vtab-eurosat</td>\n",
       "      <td>sup_vitb16_imagenet21k</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>86727946</td>\n",
       "      <td>929290</td>\n",
       "      <td>1.0715</td>\n",
       "      <td>64</td>\n",
       "      <td>100.0</td>\n",
       "      <td>96.67</td>\n",
       "      <td>41 | 100</td>\n",
       "      <td>/fsx/menglin/experiments/2022prompt/output/rel...</td>\n",
       "      <td>0 days 00:44:42</td>\n",
       "      <td>82</td>\n",
       "      <td>VPT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vtab-sun397</td>\n",
       "      <td>sup_vitb16_imagenet21k</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>86150029</td>\n",
       "      <td>351373</td>\n",
       "      <td>0.4079</td>\n",
       "      <td>128</td>\n",
       "      <td>100.0</td>\n",
       "      <td>52.45</td>\n",
       "      <td>8 | 100</td>\n",
       "      <td>/fsx/menglin/experiments/2022prompt/output/rel...</td>\n",
       "      <td>0 days 00:44:51</td>\n",
       "      <td>82</td>\n",
       "      <td>VPT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vtab-dmlab</td>\n",
       "      <td>sup_vitb16_imagenet21k</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>86724870</td>\n",
       "      <td>926214</td>\n",
       "      <td>1.0680</td>\n",
       "      <td>64</td>\n",
       "      <td>100.0</td>\n",
       "      <td>47.41</td>\n",
       "      <td>76 | 100</td>\n",
       "      <td>/fsx/menglin/experiments/2022prompt/output/rel...</td>\n",
       "      <td>0 days 01:06:15</td>\n",
       "      <td>100</td>\n",
       "      <td>VPT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vtab-eurosat</td>\n",
       "      <td>sup_vitb16_imagenet21k</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>86727946</td>\n",
       "      <td>929290</td>\n",
       "      <td>1.0715</td>\n",
       "      <td>64</td>\n",
       "      <td>100.0</td>\n",
       "      <td>95.41</td>\n",
       "      <td>47 | 100</td>\n",
       "      <td>/fsx/menglin/experiments/2022prompt/output/rel...</td>\n",
       "      <td>0 days 00:44:59</td>\n",
       "      <td>100</td>\n",
       "      <td>VPT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vtab-sun397</td>\n",
       "      <td>sup_vitb16_imagenet21k</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>86150029</td>\n",
       "      <td>351373</td>\n",
       "      <td>0.4079</td>\n",
       "      <td>128</td>\n",
       "      <td>100.0</td>\n",
       "      <td>51.08</td>\n",
       "      <td>17 | 100</td>\n",
       "      <td>/fsx/menglin/experiments/2022prompt/output/rel...</td>\n",
       "      <td>0 days 00:44:40</td>\n",
       "      <td>100</td>\n",
       "      <td>VPT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vtab-dmlab</td>\n",
       "      <td>sup_vitb16_imagenet21k</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>86724870</td>\n",
       "      <td>926214</td>\n",
       "      <td>1.0680</td>\n",
       "      <td>64</td>\n",
       "      <td>100.0</td>\n",
       "      <td>46.44</td>\n",
       "      <td>77 | 100</td>\n",
       "      <td>/fsx/menglin/experiments/2022prompt/output/rel...</td>\n",
       "      <td>0 days 01:06:12</td>\n",
       "      <td>800</td>\n",
       "      <td>VPT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vtab-eurosat</td>\n",
       "      <td>sup_vitb16_imagenet21k</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>86727946</td>\n",
       "      <td>929290</td>\n",
       "      <td>1.0715</td>\n",
       "      <td>64</td>\n",
       "      <td>100.0</td>\n",
       "      <td>96.11</td>\n",
       "      <td>42 | 100</td>\n",
       "      <td>/fsx/menglin/experiments/2022prompt/output/rel...</td>\n",
       "      <td>0 days 00:44:16</td>\n",
       "      <td>800</td>\n",
       "      <td>VPT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vtab-sun397</td>\n",
       "      <td>sup_vitb16_imagenet21k</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>86150029</td>\n",
       "      <td>351373</td>\n",
       "      <td>0.4079</td>\n",
       "      <td>128</td>\n",
       "      <td>100.0</td>\n",
       "      <td>52.72</td>\n",
       "      <td>11 | 100</td>\n",
       "      <td>/fsx/menglin/experiments/2022prompt/output/rel...</td>\n",
       "      <td>0 days 01:01:28</td>\n",
       "      <td>800</td>\n",
       "      <td>VPT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           data                 feature    lr      wd  total_params  \\\n",
       "2    vtab-dmlab  sup_vitb16_imagenet21k   1.0  0.0010      86724870   \n",
       "0  vtab-eurosat  sup_vitb16_imagenet21k   1.0  0.0010      86727946   \n",
       "1   vtab-sun397  sup_vitb16_imagenet21k  25.0  0.0001      86150029   \n",
       "2    vtab-dmlab  sup_vitb16_imagenet21k   1.0  0.0010      86724870   \n",
       "0  vtab-eurosat  sup_vitb16_imagenet21k   1.0  0.0010      86727946   \n",
       "1   vtab-sun397  sup_vitb16_imagenet21k  25.0  0.0001      86150029   \n",
       "2    vtab-dmlab  sup_vitb16_imagenet21k   1.0  0.0010      86724870   \n",
       "1  vtab-eurosat  sup_vitb16_imagenet21k   1.0  0.0010      86727946   \n",
       "0   vtab-sun397  sup_vitb16_imagenet21k  25.0  0.0001      86150029   \n",
       "2    vtab-dmlab  sup_vitb16_imagenet21k   1.0  0.0010      86724870   \n",
       "0  vtab-eurosat  sup_vitb16_imagenet21k   1.0  0.0010      86727946   \n",
       "1   vtab-sun397  sup_vitb16_imagenet21k  25.0  0.0001      86150029   \n",
       "2    vtab-dmlab  sup_vitb16_imagenet21k   1.0  0.0010      86724870   \n",
       "1  vtab-eurosat  sup_vitb16_imagenet21k   1.0  0.0010      86727946   \n",
       "0   vtab-sun397  sup_vitb16_imagenet21k  25.0  0.0001      86150029   \n",
       "\n",
       "   tuned_params  tuned / total (%)  batch_size  l-val_top1  l-test_top1  \\\n",
       "2        926214             1.0680          64       100.0        46.88   \n",
       "0        929290             1.0715          64       100.0        96.00   \n",
       "1        351373             0.4079         128       100.0        52.57   \n",
       "2        926214             1.0680          64        99.5        46.25   \n",
       "0        929290             1.0715          64       100.0        96.54   \n",
       "1        351373             0.4079         128       100.0        49.03   \n",
       "2        926214             1.0680          64       100.0        46.14   \n",
       "1        929290             1.0715          64       100.0        96.67   \n",
       "0        351373             0.4079         128       100.0        52.45   \n",
       "2        926214             1.0680          64       100.0        47.41   \n",
       "0        929290             1.0715          64       100.0        95.41   \n",
       "1        351373             0.4079         128       100.0        51.08   \n",
       "2        926214             1.0680          64       100.0        46.44   \n",
       "1        929290             1.0715          64       100.0        96.11   \n",
       "0        351373             0.4079         128       100.0        52.72   \n",
       "\n",
       "  best_epoch                                               file  \\\n",
       "2   76 | 100  /fsx/menglin/experiments/2022prompt/output/rel...   \n",
       "0   38 | 100  /fsx/menglin/experiments/2022prompt/output/rel...   \n",
       "1   14 | 100  /fsx/menglin/experiments/2022prompt/output/rel...   \n",
       "2   85 | 100  /fsx/menglin/experiments/2022prompt/output/rel...   \n",
       "0   41 | 100  /fsx/menglin/experiments/2022prompt/output/rel...   \n",
       "1   32 | 100  /fsx/menglin/experiments/2022prompt/output/rel...   \n",
       "2   65 | 100  /fsx/menglin/experiments/2022prompt/output/rel...   \n",
       "1   41 | 100  /fsx/menglin/experiments/2022prompt/output/rel...   \n",
       "0    8 | 100  /fsx/menglin/experiments/2022prompt/output/rel...   \n",
       "2   76 | 100  /fsx/menglin/experiments/2022prompt/output/rel...   \n",
       "0   47 | 100  /fsx/menglin/experiments/2022prompt/output/rel...   \n",
       "1   17 | 100  /fsx/menglin/experiments/2022prompt/output/rel...   \n",
       "2   77 | 100  /fsx/menglin/experiments/2022prompt/output/rel...   \n",
       "1   42 | 100  /fsx/menglin/experiments/2022prompt/output/rel...   \n",
       "0   11 | 100  /fsx/menglin/experiments/2022prompt/output/rel...   \n",
       "\n",
       "       total_time seed type  \n",
       "2 0 days 01:06:03   42  VPT  \n",
       "0 0 days 00:44:57   42  VPT  \n",
       "1 0 days 00:44:57   42  VPT  \n",
       "2 0 days 01:06:09   44  VPT  \n",
       "0 0 days 00:44:38   44  VPT  \n",
       "1 0 days 00:44:53   44  VPT  \n",
       "2 0 days 01:04:18   82  VPT  \n",
       "1 0 days 00:44:42   82  VPT  \n",
       "0 0 days 00:44:51   82  VPT  \n",
       "2 0 days 01:06:15  100  VPT  \n",
       "0 0 days 00:44:59  100  VPT  \n",
       "1 0 days 00:44:40  100  VPT  \n",
       "2 0 days 01:06:12  800  VPT  \n",
       "1 0 days 00:44:16  800  VPT  \n",
       "0 0 days 01:01:28  800  VPT  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = <MODEL_ROOT>\n",
    "df_list=[]\n",
    "for seed in [\"42\", \"44\", \"82\", \"100\", \"800\"]:\n",
    "#     model_type = f\"adapter_{r}\"\n",
    "    files = glob.glob(f\"{root}/seed{seed}/*/sup_vitb16_imagenet21k/*/*/{LOG_NAME}\")\n",
    "    for f in files:\n",
    "        df = get_df(files, f\"seed{seed}\", root, is_best=False, is_last=True)\n",
    "        if df is None:\n",
    "            continue\n",
    "        df[\"seed\"] = seed\n",
    "    df_list.append(df)\n",
    "\n",
    "df= pd.concat(df_list)\n",
    "df[\"type\"] = \"VPT\"\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1468233f",
   "metadata": {},
   "source": [
    "Take average of 5 runs for each dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "139f26c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>feature</th>\n",
       "      <th>type</th>\n",
       "      <th>total_runs</th>\n",
       "      <th>l-test_top1</th>\n",
       "      <th>l-test_top1-std</th>\n",
       "      <th>lr</th>\n",
       "      <th>wd</th>\n",
       "      <th>total_params</th>\n",
       "      <th>tuned_params</th>\n",
       "      <th>tuned / total (%)</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>l-val_top1</th>\n",
       "      <th>total_time</th>\n",
       "      <th>seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vtab-dmlab</td>\n",
       "      <td>sup_vitb16_imagenet21k</td>\n",
       "      <td>VPT</td>\n",
       "      <td>5</td>\n",
       "      <td>46.62</td>\n",
       "      <td>0.47</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>86724870</td>\n",
       "      <td>926214</td>\n",
       "      <td>1.0680</td>\n",
       "      <td>64</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0 days 01:06:03</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vtab-eurosat</td>\n",
       "      <td>sup_vitb16_imagenet21k</td>\n",
       "      <td>VPT</td>\n",
       "      <td>5</td>\n",
       "      <td>96.15</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>86727946</td>\n",
       "      <td>929290</td>\n",
       "      <td>1.0715</td>\n",
       "      <td>64</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0 days 00:44:57</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vtab-sun397</td>\n",
       "      <td>sup_vitb16_imagenet21k</td>\n",
       "      <td>VPT</td>\n",
       "      <td>5</td>\n",
       "      <td>51.57</td>\n",
       "      <td>1.40</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>86150029</td>\n",
       "      <td>351373</td>\n",
       "      <td>0.4079</td>\n",
       "      <td>128</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0 days 00:44:57</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           data                 feature type  total_runs l-test_top1  \\\n",
       "0    vtab-dmlab  sup_vitb16_imagenet21k  VPT           5       46.62   \n",
       "2  vtab-eurosat  sup_vitb16_imagenet21k  VPT           5       96.15   \n",
       "1   vtab-sun397  sup_vitb16_imagenet21k  VPT           5       51.57   \n",
       "\n",
       "  l-test_top1-std    lr      wd  total_params  tuned_params  \\\n",
       "0            0.47   1.0  0.0010      86724870        926214   \n",
       "2            0.45   1.0  0.0010      86727946        929290   \n",
       "1            1.40  25.0  0.0001      86150029        351373   \n",
       "\n",
       "   tuned / total (%)  batch_size  l-val_top1      total_time seed  \n",
       "0             1.0680          64       100.0 0 days 01:06:03   42  \n",
       "2             1.0715          64       100.0 0 days 00:44:57   42  \n",
       "1             0.4079         128       100.0 0 days 00:44:57   42  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LR represents the base learning rate, not the scaled one.\n",
    "f_df = average_df(df, metric_names=[\"l-test_top1\"], take_average=True)\n",
    "f_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c53a16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e49c4375",
   "metadata": {},
   "source": [
    "## tune*.py\n",
    "Tune vtab or fgvc datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c55ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Tune VTAB-caltech101 with VPT:\n",
    "python tune_vtab.py \\\n",
    "    --train-type \"prompt\" \\\n",
    "    --config-file configs/prompt/cub.yaml \\\n",
    "    MODEL.TYPE \"vit\" \\\n",
    "    DATA.BATCH_SIZE \"128\" \\\n",
    "    MODEL.PROMPT.DEEP \"True\" \\\n",
    "    MODEL.PROMPT.DROPOUT \"0.1\" \\\n",
    "    MODEL.PROMPT.NUM_TOKENS \"10\" \\\n",
    "    DATA.FEATURE \"sup_vitb16_imagenet21k\" \\\n",
    "    DATA.NAME \"vtab-caltech101\" \\\n",
    "    DATA.NUMBER_CLASSES \"102\" \\\n",
    "    DATA.DATAPATH <DATA_PATH> \\\n",
    "    MODEL.MODEL_ROOT <MODEL_ROOT> \\\n",
    "    OUTPUT_DIR <OUTPUT_PATH> \n",
    "\n",
    "# Tune CUB with VPT:\n",
    "python tune_fgvc.py \\\n",
    "    --train-type \"prompt\" \\\n",
    "    --config-file configs/prompt/cub.yaml \\\n",
    "    MODEL.TYPE \"vit\" \\\n",
    "    DATA.BATCH_SIZE \"128\" \\\n",
    "    MODEL.PROMPT.DEEP \"True\" \\\n",
    "    MODEL.PROMPT.DROPOUT \"0.1\" \\\n",
    "    MODEL.PROMPT.NUM_TOKENS \"10\" \\\n",
    "    DATA.FEATURE \"sup_vitb16_imagenet21k\" \\\n",
    "    DATA.DATAPATH <DATA_PATH> \\\n",
    "    MODEL.MODEL_ROOT <MODEL_ROOT> \\\n",
    "    OUTPUT_DIR <OUTPUT_PATH> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7abeef",
   "metadata": {},
   "source": [
    "## Backbone choices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b5f6fe",
   "metadata": {},
   "source": [
    "- Swin-B\n",
    "\n",
    "```bash\n",
    "    MODEL.TYPE \"swin\" \\\n",
    "    DATA.FEATURE \"swinb_imagenet22k_224\"\n",
    "```\n",
    "\n",
    "- ResNet-50 (VPT with prompt location == pad)\n",
    "\n",
    "```bash\n",
    "    MODEL.TYPE \"resnet\" \\\n",
    "    DATA.FEATURE \"imagenet_sup_rn50\" \\\n",
    "    SOLVER.OPTIMIZER \"sgd\" \\\n",
    "    MODEL.PROMPT.LOCATION \"pad\" \\\n",
    "    MODEL.PROMPT.NUM_TOKENS \"5\" \n",
    "```\n",
    "\n",
    "- ConvNeXt-Base (VPT with prompt location == pad)\n",
    "\n",
    "```bash\n",
    "    MODEL.TYPE \"resnext\" \\\n",
    "    DATA.FEATURE \"imagenet22k_sup_rnx_base\" \\\n",
    "    MODEL.PROMPT.LOCATION \"pad\" \\\n",
    "    MODEL.PROMPT.NUM_TOKENS \"5\" \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e15670c",
   "metadata": {},
   "source": [
    "ViT with self-supervised pre-training objectives:\n",
    "    \n",
    "- MAE\n",
    "\n",
    "```bash\n",
    "MODEL.TYPE \"ssl-vit\" \\\n",
    "DATA.FEATURE \"mae_vitb16\"\n",
    "```\n",
    "\n",
    "- MoCo-v3\n",
    "\n",
    "```bash\n",
    "MODEL.TYPE \"ssl-vit\" \\\n",
    "DATA.FEATURE \"mocov3_vitb\" \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf821823",
   "metadata": {},
   "source": [
    "## ViT (VPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ae55d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.configs.config import get_cfg\n",
    "from src.models.build_model import build_model\n",
    "import json\n",
    "model_root='/gpfs/accounts/eecs553w23_class_root/eecs553w23_class/shared_data/yourenz_group/pretrained/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bb8cc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDNN_BENCHMARK: false\n",
      "DATA:\n",
      "  BATCH_SIZE: 128\n",
      "  CLASS_WEIGHTS_TYPE: none\n",
      "  CROPSIZE: 224\n",
      "  DATAPATH: ''\n",
      "  FEATURE: sup_vitb16_imagenet21k\n",
      "  MULTILABEL: false\n",
      "  NAME: CUB\n",
      "  NO_TEST: false\n",
      "  NUMBER_CLASSES: 200\n",
      "  NUM_WORKERS: 4\n",
      "  PERCENTAGE: 1.0\n",
      "  PIN_MEMORY: true\n",
      "DBG: false\n",
      "DIST_BACKEND: nccl\n",
      "DIST_INIT_FILE: ''\n",
      "DIST_INIT_PATH: env://\n",
      "MODEL:\n",
      "  ADAPTER:\n",
      "    REDUCATION_FACTOR: 8\n",
      "    STYLE: Pfeiffer\n",
      "  LINEAR:\n",
      "    DROPOUT: 0.1\n",
      "    MLP_SIZES: []\n",
      "  MLP_NUM: 0\n",
      "  MODEL_ROOT: /gpfs/accounts/eecs553w23_class_root/eecs553w23_class/shared_data/yourenz_group/pretrained/\n",
      "  PROMPT:\n",
      "    CLSEMB_FOLDER: ''\n",
      "    CLSEMB_PATH: ''\n",
      "    DEEP: false\n",
      "    DEEP_SHARED: false\n",
      "    DROPOUT: 0.0\n",
      "    FORWARD_DEEP_NOEXPAND: false\n",
      "    INITIATION: random\n",
      "    LOCATION: prepend\n",
      "    NUM_DEEP_LAYERS: null\n",
      "    NUM_TOKENS: 5\n",
      "    PROJECT: -1\n",
      "    REVERSE_DEEP: false\n",
      "    SAVE_FOR_EACH_EPOCH: false\n",
      "    VIT_POOL_TYPE: original\n",
      "  SAVE_CKPT: false\n",
      "  TRANSFER_TYPE: prompt\n",
      "  TYPE: vit\n",
      "  WEIGHT_PATH: ''\n",
      "NUM_GPUS: 0\n",
      "NUM_SHARDS: 1\n",
      "OUTPUT_DIR: ''\n",
      "RUN_N_TIMES: 1\n",
      "SEED: null\n",
      "SOLVER:\n",
      "  BASE_LR: 0.1\n",
      "  BIAS_MULTIPLIER: 1.0\n",
      "  DBG_TRAINABLE: false\n",
      "  LOG_EVERY_N: 100\n",
      "  LOSS: softmax\n",
      "  LOSS_ALPHA: 0.01\n",
      "  MOMENTUM: 0.9\n",
      "  OPTIMIZER: sgd\n",
      "  PATIENCE: 300\n",
      "  SCHEDULER: cosine\n",
      "  TOTAL_EPOCH: 100\n",
      "  WARMUP_EPOCH: 10\n",
      "  WEIGHT_DECAY: 0.01\n",
      "  WEIGHT_DECAY_BIAS: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cfg_file = './configs/prompt/cub.yaml'\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(cfg_file)\n",
    "cfg.MODEL.MODEL_ROOT = model_root\n",
    "cfg.DATA.FEATURE = 'sup_vitb16_imagenet21k'\n",
    "print(cfg.dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be27879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import e_vit, timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe14d1f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "src.models.vit_models.ViT"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_vpt, cur_device = build_model(cfg)\n",
    "model_vpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a21c7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block input:  torch.Size([4, 202, 768])\n",
      "hidden states:  torch.Size([4, 202, 768])\n",
      "block input:  torch.Size([4, 202, 768])\n",
      "hidden states:  torch.Size([4, 202, 768])\n",
      "block input:  torch.Size([4, 202, 768])\n",
      "hidden states:  torch.Size([4, 202, 768])\n",
      "block input:  torch.Size([4, 202, 768])\n",
      "hidden states:  torch.Size([4, 202, 768])\n",
      "block input:  torch.Size([4, 202, 768])\n",
      "hidden states:  torch.Size([4, 202, 768])\n",
      "block input:  torch.Size([4, 202, 768])\n",
      "hidden states:  torch.Size([4, 202, 768])\n",
      "block input:  torch.Size([4, 202, 768])\n",
      "hidden states:  torch.Size([4, 202, 768])\n",
      "block input:  torch.Size([4, 202, 768])\n",
      "hidden states:  torch.Size([4, 202, 768])\n",
      "block input:  torch.Size([4, 202, 768])\n",
      "hidden states:  torch.Size([4, 202, 768])\n",
      "block input:  torch.Size([4, 202, 768])\n",
      "hidden states:  torch.Size([4, 202, 768])\n",
      "block input:  torch.Size([4, 202, 768])\n",
      "hidden states:  torch.Size([4, 202, 768])\n",
      "block input:  torch.Size([4, 202, 768])\n",
      "hidden states:  torch.Size([4, 202, 768])\n",
      "torch.Size([4, 200])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "input = torch.rand((4, 3, 224, 224))\n",
    "output_vpt = model_vpt(input)\n",
    "print(output_vpt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b8612c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "e_vit.patch.timm.make_tome_class.<locals>.EvitVisionTransformer"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_vit.patch.timm(model_vpt, [1,1,1,0.7,1,1,0.7,1,1,0.7,1,1])\n",
    "model_vpt.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c691038",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202\n",
      "{'keep_rate': [1, 1, 1, 0.7, 1, 1, 0.7, 1, 1, 0.7, 1, 1], 'fuse': True, 'r': [1, 1, 0.7, 1, 1, 0.7, 1, 1, 0.7, 1, 1]} 1\n",
      "202\n",
      "{'keep_rate': [1, 1, 1, 0.7, 1, 1, 0.7, 1, 1, 0.7, 1, 1], 'fuse': True, 'r': [1, 0.7, 1, 1, 0.7, 1, 1, 0.7, 1, 1]} 1\n",
      "202\n",
      "{'keep_rate': [1, 1, 1, 0.7, 1, 1, 0.7, 1, 1, 0.7, 1, 1], 'fuse': True, 'r': [0.7, 1, 1, 0.7, 1, 1, 0.7, 1, 1]} 1\n",
      "202\n",
      "{'keep_rate': [1, 1, 1, 0.7, 1, 1, 0.7, 1, 1, 0.7, 1, 1], 'fuse': True, 'r': [1, 1, 0.7, 1, 1, 0.7, 1, 1]} 0.7\n",
      "143\n",
      "{'keep_rate': [1, 1, 1, 0.7, 1, 1, 0.7, 1, 1, 0.7, 1, 1], 'fuse': True, 'r': [1, 0.7, 1, 1, 0.7, 1, 1]} 1\n",
      "143\n",
      "{'keep_rate': [1, 1, 1, 0.7, 1, 1, 0.7, 1, 1, 0.7, 1, 1], 'fuse': True, 'r': [0.7, 1, 1, 0.7, 1, 1]} 1\n",
      "143\n",
      "{'keep_rate': [1, 1, 1, 0.7, 1, 1, 0.7, 1, 1, 0.7, 1, 1], 'fuse': True, 'r': [1, 1, 0.7, 1, 1]} 0.7\n",
      "102\n",
      "{'keep_rate': [1, 1, 1, 0.7, 1, 1, 0.7, 1, 1, 0.7, 1, 1], 'fuse': True, 'r': [1, 0.7, 1, 1]} 1\n",
      "102\n",
      "{'keep_rate': [1, 1, 1, 0.7, 1, 1, 0.7, 1, 1, 0.7, 1, 1], 'fuse': True, 'r': [0.7, 1, 1]} 1\n",
      "102\n",
      "{'keep_rate': [1, 1, 1, 0.7, 1, 1, 0.7, 1, 1, 0.7, 1, 1], 'fuse': True, 'r': [1, 1]} 0.7\n",
      "73\n",
      "{'keep_rate': [1, 1, 1, 0.7, 1, 1, 0.7, 1, 1, 0.7, 1, 1], 'fuse': True, 'r': [1]} 1\n",
      "73\n",
      "{'keep_rate': [1, 1, 1, 0.7, 1, 1, 0.7, 1, 1, 0.7, 1, 1], 'fuse': True, 'r': []} 1\n",
      "torch.Size([4, 200])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "input = torch.rand((4, 3, 224, 224))\n",
    "# output_timm = model_timm(input)\n",
    "# print(output_timm.shape)\n",
    "output_vpt = model_vpt(input)\n",
    "print(output_vpt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83af076a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T_destination', ~T_destination)\n",
      "('cfg', CfgNode({'DBG': False, 'OUTPUT_DIR': '', 'RUN_N_TIMES': 1, 'CUDNN_BENCHMARK': False, 'NUM_GPUS': 0, 'NUM_SHARDS': 1, 'SEED': None, 'MODEL': CfgNode({'TRANSFER_TYPE': 'prompt', 'WEIGHT_PATH': '', 'SAVE_CKPT': False, 'MODEL_ROOT': '/gpfs/accounts/eecs598s010w23_class_root/eecs598s010w23_class/shared_data/yourenz_group/pretrained/', 'TYPE': 'vit', 'MLP_NUM': 0, 'LINEAR': CfgNode({'MLP_SIZES': [], 'DROPOUT': 0.1}), 'PROMPT': CfgNode({'NUM_TOKENS': 5, 'LOCATION': 'prepend', 'INITIATION': 'random', 'CLSEMB_FOLDER': '', 'CLSEMB_PATH': '', 'PROJECT': -1, 'DEEP': False, 'NUM_DEEP_LAYERS': None, 'REVERSE_DEEP': False, 'DEEP_SHARED': False, 'FORWARD_DEEP_NOEXPAND': False, 'VIT_POOL_TYPE': 'original', 'DROPOUT': 0.0, 'SAVE_FOR_EACH_EPOCH': False}), 'ADAPTER': CfgNode({'REDUCATION_FACTOR': 8, 'STYLE': 'Pfeiffer'})}), 'SOLVER': CfgNode({'LOSS': 'softmax', 'LOSS_ALPHA': 0.01, 'OPTIMIZER': 'sgd', 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 0.01, 'WEIGHT_DECAY_BIAS': 0, 'PATIENCE': 300, 'SCHEDULER': 'cosine', 'BASE_LR': 0.1, 'BIAS_MULTIPLIER': 1.0, 'WARMUP_EPOCH': 10, 'TOTAL_EPOCH': 100, 'LOG_EVERY_N': 100, 'DBG_TRAINABLE': False}), 'DATA': CfgNode({'NAME': 'CUB', 'DATAPATH': '', 'FEATURE': 'sup_vitb16_imagenet21k', 'PERCENTAGE': 1.0, 'NUMBER_CLASSES': 200, 'MULTILABEL': False, 'CLASS_WEIGHTS_TYPE': 'none', 'CROPSIZE': 224, 'NO_TEST': False, 'BATCH_SIZE': 128, 'NUM_WORKERS': 4, 'PIN_MEMORY': True}), 'DIST_BACKEND': 'nccl', 'DIST_INIT_PATH': 'env://', 'DIST_INIT_FILE': ''}))\n",
      "('dump_patches', False)\n",
      "('enc', PromptedVisionTransformer(\n",
      "  (transformer): PromptedTransformer(\n",
      "    (embeddings): Embeddings(\n",
      "      (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): Encoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): ToMeBlock(\n",
      "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (ffn): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (attn): ToMeAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "        )\n",
      "        (1): ToMeBlock(\n",
      "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (ffn): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (attn): ToMeAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "        )\n",
      "        (2): ToMeBlock(\n",
      "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (ffn): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (attn): ToMeAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "        )\n",
      "        (3): ToMeBlock(\n",
      "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (ffn): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (attn): ToMeAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "        )\n",
      "        (4): ToMeBlock(\n",
      "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (ffn): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (attn): ToMeAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "        )\n",
      "        (5): ToMeBlock(\n",
      "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (ffn): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (attn): ToMeAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "        )\n",
      "        (6): ToMeBlock(\n",
      "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (ffn): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (attn): ToMeAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "        )\n",
      "        (7): ToMeBlock(\n",
      "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (ffn): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (attn): ToMeAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "        )\n",
      "        (8): ToMeBlock(\n",
      "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (ffn): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (attn): ToMeAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "        )\n",
      "        (9): ToMeBlock(\n",
      "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (ffn): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (attn): ToMeAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "        )\n",
      "        (10): ToMeBlock(\n",
      "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (ffn): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (attn): ToMeAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "        )\n",
      "        (11): ToMeBlock(\n",
      "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (ffn): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (attn): ToMeAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    )\n",
      "    (prompt_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (prompt_proj): Identity()\n",
      "  )\n",
      "  (head): Identity()\n",
      "))\n",
      "('feat_dim', 768)\n",
      "('froze_enc', False)\n",
      "('head', MLP(\n",
      "  (projection): Sequential()\n",
      "  (last_layer): Linear(in_features=768, out_features=200, bias=True)\n",
      "))\n",
      "('r', 0)\n",
      "('side', None)\n",
      "('training', True)\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "for i in inspect.getmembers(model_vpt):\n",
    "     \n",
    "    # to remove private and protected\n",
    "    # functions\n",
    "    if not i[0].startswith('_'):\n",
    "         \n",
    "        # To remove other methods that\n",
    "        # doesnot start with a underscore\n",
    "        if not inspect.ismethod(i[1]):\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42acc341",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T_destination', ~T_destination)\n",
      "('blocks', Sequential(\n",
      "  (0): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU()\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (1): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU()\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (2): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU()\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (3): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU()\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (4): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU()\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (5): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU()\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (6): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU()\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (7): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU()\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (8): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU()\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (9): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU()\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (10): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU()\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (11): Block(\n",
      "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (attn): Attention(\n",
      "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (act): GELU()\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "))\n",
      "('cls_token', Parameter containing:\n",
      "tensor([[[ 3.2389e-01,  1.4362e-02, -4.3632e-01, -2.9668e-02,  4.9954e-01,\n",
      "           3.4431e-01,  6.9270e-02,  1.5145e-02,  1.5875e-01,  6.6091e-03,\n",
      "           2.9595e-02,  2.6433e-02,  4.8807e-02,  1.5438e-01,  7.6776e-02,\n",
      "          -5.6193e-02,  2.3481e+00, -3.7596e-02, -7.9648e-02, -3.2865e-02,\n",
      "           4.0753e-02,  1.0625e-01,  1.2637e-02,  1.2623e-01, -5.1822e-03,\n",
      "          -2.3552e-01,  3.1639e-02,  6.2501e-02,  3.5712e-02, -1.7925e-02,\n",
      "          -2.7995e-02,  8.2622e-01,  5.4036e-02,  3.6365e-02,  4.2601e-02,\n",
      "           9.5680e-02, -4.4193e-02,  5.1107e-02,  1.9187e-01,  3.5678e-01,\n",
      "           6.6560e-02,  8.5484e-03,  2.0514e-02,  2.7039e-02,  6.3104e-02,\n",
      "           2.8506e-02, -9.6596e-02,  9.6870e-03,  1.0163e-01,  1.4900e-01,\n",
      "           1.1239e-01,  3.5970e-02,  9.4504e-02,  3.3720e-02,  5.9837e-02,\n",
      "          -5.9888e-01,  9.4817e-02, -5.0430e-02,  3.4835e-02,  2.4602e-02,\n",
      "          -4.2094e-02,  4.2754e-02,  2.5239e-02, -1.9448e-02,  1.5616e-02,\n",
      "           1.1721e-01,  4.2430e-03,  6.4798e-02,  7.9721e-03,  9.1296e-02,\n",
      "           8.4004e-02, -1.7671e-02,  2.3556e-02,  1.8096e-01,  8.8019e-03,\n",
      "           1.5398e-01, -3.3862e-02,  3.0412e-02,  1.6411e-01, -2.0413e-01,\n",
      "          -3.6651e-03,  1.4645e-01, -2.4205e-02,  5.5545e-02,  1.7230e-01,\n",
      "           2.1083e-01,  5.8732e-02,  1.0158e-01,  3.1914e-01,  5.5121e-02,\n",
      "           4.0785e-02,  3.2414e-02,  3.1809e-01,  1.4012e-01, -7.5938e-02,\n",
      "           1.2122e-01,  1.0068e-01,  1.8242e-01,  2.7700e-02, -4.5502e-02,\n",
      "           8.2643e-02,  2.4563e-02, -2.6613e-01,  8.3892e-02,  5.3276e-02,\n",
      "           7.0566e-02,  8.1522e-02,  1.5624e+00, -1.7492e-02,  7.5047e-02,\n",
      "           1.1595e+00,  1.2600e-02,  1.3694e-01,  5.4930e-04,  3.6411e-02,\n",
      "          -1.1526e-01,  7.7863e-02,  5.9790e-02, -1.8080e-03, -2.6807e-02,\n",
      "           1.4952e-02,  9.7865e-03,  2.6364e-02, -2.1312e-01,  9.6003e-02,\n",
      "           7.9491e-02,  1.5214e-01,  1.5762e-01,  7.9580e-02,  1.0120e+00,\n",
      "           8.6344e-02, -3.0504e-02,  4.2529e-02,  2.3245e-03,  1.2629e-01,\n",
      "           1.9314e-02,  5.2183e-02,  5.8595e-02, -2.7113e-02,  7.2653e-02,\n",
      "          -4.6519e-03,  4.3101e-02, -5.2659e-01, -1.4413e-01,  6.6794e-02,\n",
      "           1.1696e-01,  4.8844e-02, -2.9169e-02, -7.5423e-01, -1.2439e-02,\n",
      "           1.4604e-01, -5.7047e-01,  2.0771e-01,  1.2454e-01,  4.3423e-02,\n",
      "           5.7994e-02,  4.5551e-02,  3.6124e-03,  4.3215e-01,  5.2314e-02,\n",
      "           4.4464e-03,  9.3944e-02, -1.0045e-02, -3.2612e-02,  7.9689e-02,\n",
      "           6.7831e-03,  4.4144e-02,  4.0492e-02, -2.9282e-02,  3.8727e-02,\n",
      "           1.4788e-01,  8.1222e-02, -4.5352e-02,  1.3985e-01,  9.9450e-02,\n",
      "           1.0526e-01,  1.2710e-02, -3.8683e-03,  9.6250e-02,  1.0717e-02,\n",
      "           9.8245e-02,  4.8597e-02, -7.2989e-02,  2.8915e-02, -8.6425e-01,\n",
      "          -1.4071e-02,  7.4777e-02,  1.7159e-01, -4.5008e-02, -1.6974e-01,\n",
      "           3.5332e-02,  1.6560e-02,  6.8398e-03, -1.1974e-01,  7.2733e-02,\n",
      "          -1.8558e-02,  8.1849e-02, -6.0576e-02, -1.7302e-02,  2.9005e-01,\n",
      "           5.6656e-01,  1.9566e-02,  2.4736e-02,  2.1657e-02,  1.5146e-01,\n",
      "          -3.5062e-03,  1.4018e-01, -6.5184e-03, -1.8303e-02, -4.5030e-02,\n",
      "           1.7889e-02,  3.9680e-02,  3.1711e-02,  1.2308e-02,  3.8764e-02,\n",
      "           5.3699e-03, -1.9445e-01,  2.6532e-02, -9.2788e-01,  5.7252e-02,\n",
      "          -6.1909e-02,  9.5356e-02,  1.6284e-02,  1.2930e-01,  8.1339e-03,\n",
      "           1.6943e-01, -1.1838e-01, -1.5950e-02, -1.1015e-01,  1.3522e-02,\n",
      "           4.1448e-02,  8.8799e-02,  5.2903e-02,  1.9341e-01,  2.5800e-02,\n",
      "           4.0231e-02,  1.1141e-02, -7.3028e-02,  7.1541e-02,  5.3180e-03,\n",
      "           8.2791e-02,  2.8758e-02,  7.1109e-02,  3.0477e-02,  9.1477e-03,\n",
      "           9.9797e-02,  2.2127e-02,  1.2448e-01, -9.3052e-02, -9.0740e-02,\n",
      "          -5.8852e-01,  7.6129e-02,  3.4690e-03,  5.9210e-02, -7.9035e-03,\n",
      "          -1.2725e-01,  5.2775e-01,  1.0837e-02, -5.8583e-02, -1.9944e-03,\n",
      "           6.9210e-02,  5.7270e-02,  5.2204e-02,  5.4136e-02,  1.2421e-01,\n",
      "          -4.4489e-02,  9.0958e-02,  7.3893e-02, -2.1665e-02,  2.3248e-02,\n",
      "           3.3151e-02,  8.8333e-02, -7.9575e-02,  3.9092e-02,  6.0476e-02,\n",
      "          -6.2437e-03,  1.3049e-02,  7.2333e-01, -1.8323e-01,  5.4341e-02,\n",
      "           1.2472e-02, -1.2755e-02,  1.7448e-01,  8.6896e-02,  1.3468e-01,\n",
      "          -9.7702e-03,  4.4243e-02,  2.4137e-01,  5.7908e-02,  1.5712e-01,\n",
      "          -1.6281e-02,  1.1650e-01,  2.7980e-02,  3.1462e-02,  1.7244e-01,\n",
      "          -1.7302e-02, -1.5033e+00,  8.0659e-02, -2.6041e-02, -1.8317e-04,\n",
      "          -1.2001e-02, -5.2800e-02, -5.7993e-02,  4.8893e-02,  1.5633e-02,\n",
      "           2.0852e-02,  5.0203e-02,  2.0539e-02, -4.2537e-02, -1.7607e-02,\n",
      "           7.7612e-02, -5.1773e-02,  8.0125e-02,  1.7938e-02, -3.3657e+00,\n",
      "           6.9630e-01, -8.7460e-03,  1.0587e-01,  1.5665e-01, -4.7122e-03,\n",
      "          -4.8021e-04,  2.7476e-02, -2.4608e-01,  1.7730e+00,  5.0381e-01,\n",
      "          -1.4659e-01,  9.9170e-02,  2.0950e-02,  7.8500e-02,  4.5920e-02,\n",
      "          -1.3854e-02,  3.0543e-02, -2.0931e-01,  1.7636e-02,  1.8623e-01,\n",
      "           3.2601e-02,  3.5952e-02, -6.4363e-02,  7.3958e-02, -7.1535e-02,\n",
      "           5.6311e-02,  2.5744e-01, -5.1392e-02,  6.6536e-02,  4.1959e-02,\n",
      "           8.0788e-02,  5.6096e-03,  5.6674e-02,  4.6891e-02,  1.2995e-02,\n",
      "           3.2255e-02,  7.5610e-02,  4.0349e-01, -3.8133e-02,  7.3580e-02,\n",
      "           6.6140e-02, -1.3434e-02,  2.0061e-01, -1.9098e-02, -7.5313e-03,\n",
      "           2.2288e-01,  1.2997e+00,  3.8796e-02, -2.5732e-02,  4.9975e-02,\n",
      "          -1.1274e-02,  4.7729e-02,  1.9753e-02,  1.0158e-01,  5.8760e-02,\n",
      "          -1.0071e-03,  9.2439e-02,  2.1413e-02,  1.2822e-01, -2.8954e-01,\n",
      "           9.0068e-02,  7.7865e-02,  2.4180e-01, -1.7035e-01,  6.0370e-03,\n",
      "           3.1061e-02, -3.1637e-02, -8.2226e-02,  2.0363e-02, -1.0456e-02,\n",
      "          -1.9296e-02,  3.8091e-02, -1.0147e-02,  8.7945e-02,  1.3347e-01,\n",
      "          -2.6594e-02,  2.7395e-02,  5.3992e-02,  6.5324e-04,  3.4630e-02,\n",
      "          -2.6857e-01,  1.5598e-02,  3.3938e-02,  3.6477e-01,  3.9252e-02,\n",
      "           1.1554e-01,  3.4003e-02, -9.0701e-02, -3.7073e-02,  3.9404e-02,\n",
      "           1.9156e+00,  2.3203e-02, -3.0133e-03, -3.6937e-02,  7.9992e-01,\n",
      "           7.4559e-02, -4.1759e-03, -1.9962e-03, -4.1466e-02,  6.6701e-01,\n",
      "           6.1394e-02,  6.0081e-02,  2.4160e+00,  4.2351e-02,  2.8445e-02,\n",
      "           3.4370e-02, -2.7724e-03,  1.1965e-02, -1.0361e-01, -1.8391e-03,\n",
      "           3.8600e-02, -2.8997e-02,  5.5437e-03,  1.4194e-01,  5.4073e-01,\n",
      "          -5.1722e-01,  1.4051e-01,  4.6270e-02,  5.4753e-02,  5.6229e-02,\n",
      "          -1.2004e-01, -4.6945e-01, -3.2054e-02, -3.3034e-01,  9.8761e-02,\n",
      "           2.0538e-01,  1.0939e-01,  7.7358e-02,  3.3519e-02, -3.8565e-02,\n",
      "           4.9140e-02,  5.1205e-02,  1.8941e-01,  1.9740e-02, -2.1110e-02,\n",
      "           7.4486e-02, -3.1521e-02, -6.1928e-02,  7.5666e-02, -1.3296e-02,\n",
      "           2.4949e-02,  5.8097e-03, -8.8280e-01,  1.3590e-02, -5.5627e-02,\n",
      "          -7.9828e-03,  5.9798e-02,  7.3760e-02, -8.9443e-02,  3.2033e-02,\n",
      "           9.1112e-02,  6.4500e-02, -2.6404e-01,  1.6110e-02, -4.3450e-02,\n",
      "          -3.7405e-03, -5.8664e-02, -9.3763e-03, -2.4467e-02, -1.3759e-01,\n",
      "           1.4886e-02,  7.9728e-02,  4.5801e-02, -8.3435e-02,  7.2693e-02,\n",
      "          -2.7132e-02,  7.0447e-02,  1.7907e-01,  1.3892e-02,  2.8792e-02,\n",
      "           2.2253e-01,  4.5282e-03,  4.0711e-02,  3.1366e-01,  1.2602e-03,\n",
      "           5.7367e-02,  9.3153e-03,  5.7622e-02,  2.3959e-01, -2.4253e-02,\n",
      "           6.3766e-02,  1.2189e-01, -2.8474e-03,  7.5160e-02,  1.8352e-02,\n",
      "           1.1177e-02,  9.0674e-02,  1.1800e-01,  8.1226e-03, -1.3877e-01,\n",
      "           5.0215e-03,  1.3445e-02,  3.1177e-02,  8.4751e-02, -1.9023e-03,\n",
      "          -2.2479e-01,  7.2646e-02,  2.2689e-02,  2.8530e-02,  6.1618e-02,\n",
      "           1.0659e-02, -1.5065e+00,  5.3842e-02,  6.0718e-02, -6.1831e-02,\n",
      "           2.0735e-02,  2.5973e-02,  3.4282e-02, -9.3750e+00, -8.8286e-02,\n",
      "           5.7395e-02,  1.0037e-01,  3.1012e-02,  1.1930e-01,  2.5022e-01,\n",
      "           2.5833e-01,  2.6478e-01, -1.5174e-01,  1.5214e+00, -1.4572e-01,\n",
      "           6.9015e-01,  5.6231e-02, -7.2695e-01,  6.3875e-02,  4.4186e-02,\n",
      "          -2.3787e-01, -1.2321e-01, -1.4463e-01,  1.0808e-01,  1.8293e-02,\n",
      "           1.0522e-01,  3.1962e-01, -9.4648e-02,  6.8689e-02, -8.7037e-02,\n",
      "           2.1733e-01,  8.3254e-01,  3.0565e-02,  3.1025e-02,  1.0224e-01,\n",
      "          -7.6582e-02, -8.6448e-03,  4.4948e-03,  1.1537e-01,  2.3789e-02,\n",
      "          -4.4557e-03,  7.4323e-02, -2.3297e-02, -1.2272e-01, -3.9034e-01,\n",
      "          -6.8277e-02, -8.0043e-03,  1.2705e+00, -2.9728e+00,  3.1505e-01,\n",
      "           2.3009e-02, -2.9453e-02, -3.2787e-02,  1.2791e-01,  2.5345e-01,\n",
      "           4.8279e-03, -1.0550e+00,  1.0010e-02,  5.0660e-02, -2.0874e-01,\n",
      "          -2.0686e-02, -6.0266e-02,  1.0924e-01, -2.2803e+00,  4.8846e-02,\n",
      "          -9.9760e-01,  1.6951e-02, -1.5022e-01,  8.8568e-02,  2.9742e-02,\n",
      "          -1.5317e-02,  3.1720e-01,  1.9445e-01,  6.3562e-02,  5.3272e-02,\n",
      "           8.4596e-02,  1.5648e-01, -4.0606e-02, -3.2457e-03, -3.0240e-02,\n",
      "           1.0925e-01,  4.0707e-02, -3.4337e-02, -1.3420e-02,  3.9623e-02,\n",
      "          -3.4799e-02, -1.2241e-02, -6.1390e-03, -5.7148e-02,  1.1977e-03,\n",
      "           6.2572e-02,  2.8121e-02, -6.5980e-02,  8.2090e-02, -1.2545e+00,\n",
      "           7.0753e-02,  2.0042e-01,  3.2258e-02,  1.8815e-01,  6.9678e-02,\n",
      "          -1.3492e-01,  1.2644e-02, -1.0510e-01,  4.9939e-02,  3.0231e-02,\n",
      "           2.2881e-01,  1.1035e-02,  4.6823e-01,  4.7932e-01,  1.2302e+00,\n",
      "           2.9543e-02, -2.1057e-01, -7.4377e-02, -1.6244e-01,  8.4742e-03,\n",
      "           6.3245e-02,  3.8060e-02,  8.3000e-01,  6.2296e-04, -9.0807e-02,\n",
      "           7.4830e-02, -4.5585e-02,  3.3298e-02,  3.1276e-03,  1.2091e-02,\n",
      "           6.7059e-03,  1.5346e-02,  5.7833e-02,  1.3322e-01,  2.6463e-02,\n",
      "          -2.5677e-02,  4.1164e-02, -1.5903e-01, -7.3745e-02,  2.3360e-01,\n",
      "           1.2467e-01, -6.7162e-01,  1.1573e-01,  9.7077e-02,  8.3083e-02,\n",
      "          -9.7547e-01,  4.7309e-02,  2.2063e-02,  2.8144e-02,  4.6663e-02,\n",
      "          -1.1288e-01, -7.9621e-03,  4.2650e-02, -3.4649e-02, -5.1988e-01,\n",
      "           2.8083e-02,  4.0305e-03,  9.3621e-02,  4.7034e-02,  8.3521e-02,\n",
      "           7.4444e-02, -2.0172e-02,  5.4925e-02, -5.3026e-02,  1.4037e-02,\n",
      "           2.2063e-02,  8.7021e-02, -5.5030e-02,  3.9415e-02, -2.7496e-02,\n",
      "           2.8837e-02,  3.6025e-02, -3.6240e-02,  1.5174e-02, -1.8619e-03,\n",
      "           3.5814e-02,  6.2581e-02, -2.6211e-02, -2.1863e-01, -1.1890e-02,\n",
      "           8.7036e-02,  6.2867e-02,  6.3943e-02,  2.3341e-01, -2.3486e-02,\n",
      "           5.9747e-02,  1.6464e-02,  3.3283e-02, -1.0433e-01,  1.9238e-01,\n",
      "           3.7589e-02,  1.3944e-02,  4.5698e-02,  1.8592e-02, -3.7619e-02,\n",
      "          -6.8713e-02,  8.3707e-02,  2.6413e-02,  1.6667e-02,  7.0701e-02,\n",
      "           2.8522e-02, -2.8036e-02, -8.6024e-02,  6.0961e-02, -6.0156e-02,\n",
      "           1.3152e-01,  7.0211e-02,  1.8001e-02,  2.1084e-03,  7.9305e-03,\n",
      "           6.4065e-01,  1.5428e+00, -7.7079e-01,  2.7518e-01,  2.8064e-02,\n",
      "           8.2189e-02,  7.1246e-02, -2.6515e-02, -3.3438e-02,  4.5829e-01,\n",
      "          -1.8265e-01,  1.2518e-02,  3.3975e-03,  1.3161e-02,  1.0637e-01,\n",
      "           4.3740e-01,  1.1834e-01,  2.1136e-01,  2.4477e-02,  9.6829e-02,\n",
      "           3.3614e-02,  5.9703e-02, -2.9448e-01, -1.1395e-02, -3.9123e-02,\n",
      "           1.3692e-01,  8.6118e-02,  2.9678e-02,  3.9886e-02,  2.2495e-01,\n",
      "          -5.8475e-02, -2.1268e-02,  1.8489e-01,  2.4749e-02,  5.5768e-02,\n",
      "          -2.4590e-02,  3.3249e-02,  1.2121e-01,  5.7284e-02,  2.9763e-02,\n",
      "           7.6478e-02,  2.6607e-02, -7.8469e-02]]], requires_grad=True))\n",
      "('default_cfg', {'url': 'https://storage.googleapis.com/vit_models/augreg/B_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.01-res_224.npz', 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None, 'crop_pct': 0.9, 'interpolation': 'bicubic', 'fixed_input_size': True, 'mean': (0.5, 0.5, 0.5), 'std': (0.5, 0.5, 0.5), 'first_conv': 'patch_embed.proj', 'classifier': 'head', 'architecture': 'vit_base_patch16_224'})\n",
      "('dist_token', None)\n",
      "('dump_patches', False)\n",
      "('embed_dim', 768)\n",
      "('head', Linear(in_features=768, out_features=1000, bias=True))\n",
      "('head_dist', None)\n",
      "('norm', LayerNorm((768,), eps=1e-06, elementwise_affine=True))\n",
      "('num_classes', 1000)\n",
      "('num_features', 768)\n",
      "('num_tokens', 1)\n",
      "('patch_embed', PatchEmbed(\n",
      "  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "  (norm): Identity()\n",
      "))\n",
      "('pos_drop', Dropout(p=0.0, inplace=False))\n",
      "('pos_embed', Parameter containing:\n",
      "tensor([[[ 3.0645e-01,  4.1925e-02, -4.5591e-01,  ...,  6.0291e-02,\n",
      "           3.0391e-02, -1.1641e-01],\n",
      "         [ 2.4408e-02,  1.6084e-03, -1.2531e+00,  ..., -2.4456e-01,\n",
      "           8.4392e-03, -1.4195e-01],\n",
      "         [-2.5824e-02,  6.4096e-03, -2.0030e+00,  ..., -3.1602e-01,\n",
      "           1.0344e-02, -1.1887e-01],\n",
      "         ...,\n",
      "         [-3.7619e-03, -5.0256e-03, -2.5499e-01,  ...,  2.0253e-01,\n",
      "           1.1322e-03, -1.0075e-01],\n",
      "         [ 7.2607e-03, -8.8217e-04, -3.5149e-01,  ...,  1.3258e-01,\n",
      "           3.2877e-03, -9.7187e-02],\n",
      "         [ 9.0165e-02, -9.7315e-04,  3.0878e-01,  ...,  1.9121e-01,\n",
      "          -4.9151e-04, -8.6742e-02]]], requires_grad=True))\n",
      "('pre_logits', Identity())\n",
      "('training', True)\n"
     ]
    }
   ],
   "source": [
    "for i in inspect.getmembers(model_timm):\n",
    "     \n",
    "    # to remove private and protected\n",
    "    # functions\n",
    "    if not i[0].startswith('_'):\n",
    "         \n",
    "        # To remove other methods that\n",
    "        # doesnot start with a underscore\n",
    "        if not inspect.ismethod(i[1]):\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebef175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "        # k: [b, num_heads, N, dim // num_heads]\n",
    "        print(k.shape)\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "658a2563",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand((4, 10, 72))\n",
    "attn = Attention(dim=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55a67734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 10, 9])\n",
      "torch.Size([4, 10, 72])\n"
     ]
    }
   ],
   "source": [
    "ouput = attn(input)\n",
    "print(ouput.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d02440",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.12cuda11.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
